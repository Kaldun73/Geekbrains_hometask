{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ckn-aPffOHkz",
        "outputId": "f3eafc3f-a35e-43bc-9f54-b5f358afacf2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pyconll\n",
        "\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger, SequentialBackoffTagger\n",
        "from nltk.corpus import names\n",
        "import nltk\n",
        "nltk.download('names')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkpcHsV8RWHA"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAQBOJRARev7"
      },
      "source": [
        "**Написать теггер на данных с руским языком**\n",
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации  \n",
        "2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки  \n",
        "3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  \n",
        "4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)    \n",
        "5. сравнить все реализованные методы сделать выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_16J0ER8WOJx"
      },
      "source": [
        "## загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPRx8Cu_RDY1",
        "outputId": "10dc19b0-242f-4077-d9b8-ac51d201bc2e"
      },
      "source": [
        "!pip install pyconll"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wgL-33mWUyZ"
      },
      "source": [
        "import pyconll"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXxwW9NzW570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed6ef0f-00d2-4d47-8426-61fcf465d097"
      },
      "source": [
        "!mkdir datasets"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpwgA3svWiRw",
        "outputId": "2259fc32-7c35-40d5-dee4-fee0780e333e"
      },
      "source": [
        "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 17:42:29--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81043533 (77M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  77.29M   103MB/s    in 0.7s    \n",
            "\n",
            "2021-04-12 17:42:30 (103 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81043533/81043533]\n",
            "\n",
            "--2021-04-12 17:42:30--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10903424 (10M) [text/plain]\n",
            "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./datasets/ru_synta 100%[===================>]  10.40M  34.0MB/s    in 0.3s    \n",
            "\n",
            "2021-04-12 17:42:31 (34.0 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10903424/10903424]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oymo30RBWjjl"
      },
      "source": [
        "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp-f62soI5u1"
      },
      "source": [
        "def convert_data(dataset):\n",
        "    result = []\n",
        "\n",
        "    for sent in dataset:\n",
        "        for token in sent:\n",
        "            result.append((token.form.lower(), token.upos))\n",
        "            \n",
        "    return result"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OshO48XLXQar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a1b912-cd64-47a3-cba8-b3d0cb36ce51"
      },
      "source": [
        "train_data = convert_data(full_train) \n",
        "train_data[:5]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('анкета', 'NOUN'),\n",
              " ('.', 'PUNCT'),\n",
              " ('начальник', 'NOUN'),\n",
              " ('областного', 'ADJ'),\n",
              " ('управления', 'NOUN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxYsLksxD4oY",
        "outputId": "7967ec3e-336f-4795-d1e4-738f4891d3e4"
      },
      "source": [
        "test_data = convert_data(full_test) \n",
        "test_data[:5]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('алгоритм', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u9h80OaNthc"
      },
      "source": [
        "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeAsN93THAlC"
      },
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj4tV8ytXTry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "d8967e56-d346-42cc-f841-2d19f72db2a9"
      },
      "source": [
        "df_score = pd.DataFrame(columns=['values', 'UnigramTagger', 'BigramTagger', 'TrigramTagger', 'bi_unigram_tagger', 'tri_bigram_tagger','tri_bi_unigram_tagger']).set_index('values')\n",
        "COUNT = (1000, 2000, 4000, 6000, 8000, 12000, 14000, 30000)\n",
        "\n",
        "for C in COUNT:\n",
        "    unigram_tagger = UnigramTagger([train_data[:C]])\n",
        "    bigram_tagger = BigramTagger([train_data[:C]])\n",
        "    trigram_tagger = TrigramTagger([train_data[:C]])\n",
        "    bi_unigram_tagger = BigramTagger([train_data[:C]], backoff=unigram_tagger)\n",
        "    tri_bigram_tagger = TrigramTagger([train_data[:C]], backoff=bigram_tagger)\n",
        "    tri_bi_unigram_tagger = TrigramTagger([train_data[:C]], backoff=bi_unigram_tagger)\n",
        "\n",
        "    df_score.loc[C, 'UnigramTagger'] = unigram_tagger.evaluate([test_data])\n",
        "    df_score.loc[C, 'BigramTagger'] = bigram_tagger.evaluate([test_data])\n",
        "    df_score.loc[C, 'TrigramTagger'] = trigram_tagger.evaluate([test_data])\n",
        "    df_score.loc[C, 'bi_unigram_tagger'] = bi_unigram_tagger.evaluate([test_data])\n",
        "    df_score.loc[C, 'tri_bigram_tagger'] = tri_bigram_tagger.evaluate([test_data])\n",
        "    df_score.loc[C, 'tri_bi_unigram_tagger'] = tri_bi_unigram_tagger.evaluate([test_data])\n",
        "\n",
        "df_score\n",
        "    "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UnigramTagger</th>\n",
              "      <th>BigramTagger</th>\n",
              "      <th>TrigramTagger</th>\n",
              "      <th>bi_unigram_tagger</th>\n",
              "      <th>tri_bigram_tagger</th>\n",
              "      <th>tri_bi_unigram_tagger</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>values</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>0.399951</td>\n",
              "      <td>0.00174401</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.401139</td>\n",
              "      <td>0.00174401</td>\n",
              "      <td>0.401164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>0.449963</td>\n",
              "      <td>0.00174401</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.451235</td>\n",
              "      <td>0.00174401</td>\n",
              "      <td>0.451429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4000</th>\n",
              "      <td>0.489064</td>\n",
              "      <td>0.00174401</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.490935</td>\n",
              "      <td>0.00174401</td>\n",
              "      <td>0.491111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6000</th>\n",
              "      <td>0.516741</td>\n",
              "      <td>0.00198834</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.51728</td>\n",
              "      <td>0.00198834</td>\n",
              "      <td>0.517356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8000</th>\n",
              "      <td>0.525351</td>\n",
              "      <td>0.00722879</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.527281</td>\n",
              "      <td>0.00723722</td>\n",
              "      <td>0.52739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12000</th>\n",
              "      <td>0.548243</td>\n",
              "      <td>0.0401206</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.549843</td>\n",
              "      <td>0.0401459</td>\n",
              "      <td>0.550113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14000</th>\n",
              "      <td>0.565084</td>\n",
              "      <td>0.080157</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.567755</td>\n",
              "      <td>0.079997</td>\n",
              "      <td>0.568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30000</th>\n",
              "      <td>0.643573</td>\n",
              "      <td>0.118449</td>\n",
              "      <td>0.00171873</td>\n",
              "      <td>0.635477</td>\n",
              "      <td>0.118424</td>\n",
              "      <td>0.635957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       UnigramTagger BigramTagger  ... tri_bigram_tagger tri_bi_unigram_tagger\n",
              "values                             ...                                        \n",
              "1000        0.399951   0.00174401  ...        0.00174401              0.401164\n",
              "2000        0.449963   0.00174401  ...        0.00174401              0.451429\n",
              "4000        0.489064   0.00174401  ...        0.00174401              0.491111\n",
              "6000        0.516741   0.00198834  ...        0.00198834              0.517356\n",
              "8000        0.525351   0.00722879  ...        0.00723722               0.52739\n",
              "12000       0.548243    0.0401206  ...         0.0401459              0.550113\n",
              "14000       0.565084     0.080157  ...          0.079997                 0.568\n",
              "30000       0.643573     0.118449  ...          0.118424              0.635957\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEVUEJwpMoso"
      },
      "source": [
        "Лучшие результаты получились у моделей, где использован UnigramTagger. При этом комбинации лучше, но ненамного. TrigramTagger справляется хуже всего, но позволяет чуть-чуть улучшить сочетание Bi и Uni в последнем варианте. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHT7HUQyOchz"
      },
      "source": [
        "2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaKL4F8zQiYJ"
      },
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvkLU6dOPQVn"
      },
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    for tok in sent:\n",
        "        train_tok.append(tok[0])\n",
        "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    for tok in sent:\n",
        "        test_tok.append(tok[0])\n",
        "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPJUGhxJMrOC"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ohpy66jMrK5"
      },
      "source": [
        "le = LabelEncoder()\n",
        "train_enc_labels = le.fit_transform(train_label)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obzRa0C7MrH4"
      },
      "source": [
        "test_enc_labels = le.transform(test_label)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFzRFLFhMrEm",
        "outputId": "148d59ce-bb0f-4f6f-cec8-1c64b5ed335c"
      },
      "source": [
        "le.classes_"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
              "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
              "       'VERB', 'X'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgBk_YHRMrBX"
      },
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=100)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upWZgvCNMq-i"
      },
      "source": [
        "X_train = hvectorizer.fit_transform(train_tok)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcg6bXjnMq7c"
      },
      "source": [
        "X_test = hvectorizer.transform(test_tok)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpCNv9k4Mq4Y",
        "outputId": "a0db0cba-0ab6-470b-8fd3-a37e6ec1bfed"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(871526, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLZLWFIvMq1e",
        "outputId": "6c82b18d-980d-40c8-9f80-e18b4fa60670"
      },
      "source": [
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(X_train, train_enc_labels)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQA4LWFTMqyd"
      },
      "source": [
        "pred = lr.predict(X_test)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIvX5xWTMqvi",
        "outputId": "dfbe4681-ac81-4edd-9d70-01de7adfb1d8"
      },
      "source": [
        "accuracy_score(test_enc_labels, pred)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.685454790550332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qp9RETQMqpo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3_FZQ2vMqmk"
      },
      "source": [
        "cvectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char', max_features=100)\n",
        "Xc_train = cvectorizer.fit_transform(train_tok)\n",
        "Xc_test = cvectorizer.transform(test_tok)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STbB4A52RPbv",
        "outputId": "6c0abf3b-fe56-4e22-f2df-ab31d9c0fe94"
      },
      "source": [
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(Xc_train, train_enc_labels)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_g48zKERTlN"
      },
      "source": [
        "pred_c = lr.predict(Xc_test)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyI7-9M2RVZI",
        "outputId": "122b247c-df88-4921-8bd9-fd35c5515410"
      },
      "source": [
        "accuracy_score(test_enc_labels, pred_c)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7769689616823375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-zlxUz8SSXq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESbIv8kLSSUj"
      },
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(1, 5), analyzer='word', n_features=100)\n",
        "Xhw_train = hvectorizer.fit_transform(train_tok)\n",
        "Xhw_test = hvectorizer.transform(test_tok)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORHoBQMxSSRi",
        "outputId": "ed362a98-0714-40b5-ab09-91da0d491091"
      },
      "source": [
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(Xhw_train, train_enc_labels)\n",
        "pred_hw = lr.predict(Xhw_test)\n",
        "accuracy_score(test_enc_labels, pred_hw)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.288326087689145"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ttoFXwnSSO1",
        "outputId": "6b26a088-ce02-4f90-d04d-3ffa3ec1b4cd"
      },
      "source": [
        "cvectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='word', max_features=100)\n",
        "Xcw_train = cvectorizer.fit_transform(train_tok)\n",
        "Xcw_test = cvectorizer.transform(test_tok)\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(Xcw_train, train_enc_labels)\n",
        "pred_cw = lr.predict(Xcw_test)\n",
        "accuracy_score(test_enc_labels, pred_cw)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.382081353418933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkaf037BSSLv",
        "outputId": "76c392e7-7c2a-4f87-eb0d-1e736d2cb307"
      },
      "source": [
        "hvectorizer = HashingVectorizer(ngram_range=(1, 5), analyzer='char_wb', n_features=100)\n",
        "Xhcw_train = hvectorizer.fit_transform(train_tok)\n",
        "Xhcw_test = hvectorizer.transform(test_tok)\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(Xhcw_train, train_enc_labels)\n",
        "pred_hcw = lr.predict(Xhcw_test)\n",
        "accuracy_score(test_enc_labels, pred_hcw)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7333265931992047"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFElhU_wSSIv",
        "outputId": "4911cc8a-474e-4bb4-e4bd-29899d74666b"
      },
      "source": [
        "cvectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char_wb', max_features=100)\n",
        "Xccw_train = cvectorizer.fit_transform(train_tok)\n",
        "Xccw_test = cvectorizer.transform(test_tok)\n",
        "lr = LogisticRegression(random_state=0)\n",
        "lr.fit(Xccw_train, train_enc_labels)\n",
        "pred_ccw = lr.predict(Xccw_test)\n",
        "accuracy_score(test_enc_labels, pred_ccw)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7749806221143801"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdTLbW-dSA3J"
      },
      "source": [
        "3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxi5YNZ1SAzu"
      },
      "source": [
        "4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aNWToi0Kvumi",
        "outputId": "58a14647-ba9d-46e1-95c0-8ff4bb0a8c2b"
      },
      "source": [
        "df_score_vec = pd.DataFrame(columns=['vectorizer', 'Tagger']).set_index('vectorizer')\n",
        "\n",
        "for window in (3, 5, 9, 15):\n",
        "  for emb_key, emd_val in {'W2V': Word2Vec(sentences=[train_tok[:2000] + test_tok], size=300, window=window, min_count=1), \n",
        "                           'FT': FastText(sentences=[train_tok[:2000] + test_tok], size=300, window=window, min_count=1)}.items():\n",
        "\n",
        "    X_train = np.array([emd_val[word] for word in train_tok[:2000]])\n",
        "    X_test = np.array([emd_val[word] for word in test_tok])\n",
        "\n",
        "    lr = LogisticRegression()\n",
        "    lr.fit(X_train, train_enc_labels[:2000])\n",
        "    y_pred = lr.predict(X_test)\n",
        "\n",
        "    df_score_vec.loc[f'{emb_key} {window}', 'Tagger'] = accuracy_score(test_enc_labels, y_pred)\n",
        "\n",
        "df_score_vec.plot(kind='barh', grid=True, title='Score', figsize=(10,4))\n",
        "df_score_vec"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CastomTagger</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vectorizer</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>W2V 3</th>\n",
              "      <td>0.414257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FT 3</th>\n",
              "      <td>0.364102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V 5</th>\n",
              "      <td>0.412926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FT 5</th>\n",
              "      <td>0.365838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V 9</th>\n",
              "      <td>0.419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FT 9</th>\n",
              "      <td>0.369073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>W2V 15</th>\n",
              "      <td>0.424797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FT 15</th>\n",
              "      <td>0.372999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           CastomTagger\n",
              "vectorizer             \n",
              "W2V 3          0.414257\n",
              "FT 3           0.364102\n",
              "W2V 5          0.412926\n",
              "FT 5           0.365838\n",
              "W2V 9             0.419\n",
              "FT 9           0.369073\n",
              "W2V 15         0.424797\n",
              "FT 15          0.372999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAEICAYAAADMXYlmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZiXBZ3v8fcHGJgRhocCSVFBVg0bURRChcoZKaWwB48t6tom7Rbatnk08uieOlhtrbTXRWLHHlY7+bxE2WVZaloboy0PJeDAYKipjImsD4DpoCAyfM8fv3vYn+MMM/Cb39wz9/15Xdfv4n6+vx9urp9f76efIgIzMzMzy45+aRdgZmZmZt3LDZ6ZmZlZxrjBMzMzM8sYN3hmZmZmGeMGz8zMzCxj3OCZmZmZZYwbPDMzM7OMcYNnZtYFkt4jabmklyVtk7RM0rvTrsvMrD0D0i7AzKy3kzQU+CXwWeDHwEDgvcDr3biP/hHR0l3bM7N88xk8M7POHQMQEYsjoiUidkTE/RGxDkDSZyRtkNQs6Y+STkqmHyupXtJfJD0i6SOtG5R0k6TvSbpH0qtAnaRDJf1U0ouSNkq6JJW0ZtbnucEzM+vc40CLpJslfVDSiNYZkv4a+ArwSWAo8BFgq6QK4BfA/cDBwOeB2yW9s2i7fwN8A6gGlifLrwXGADOASyWdWeZsZpZBbvDMzDoREa8A7wECuAF4UdJdkkYDnwb+NSIeioInIuJp4BRgCLAgInZFxG8pXOY9v2jTP4+IZRGxB5gIjIqIryXLP5Xs67yeS2pmWeF78MzMuiAiNgBzACRNAG4DFgGHA0+2s8qhwDNJ89bqaQpn51o9UzQ8FjhU0l+KpvUHfldy8WaWO27wzMz2U0Q8Kukm4CIKTdpftbPYZuBwSf2KmrwjKFzu3bupouFngI0RcXQZSjaznPElWjOzTkiaIGmepMOS8cMpXGpdCfwA+KKkySo4StJY4PfAa8D/klQhqRb4MPCjDnbzB6BZ0hWSqiT1l3ScX8ViZgfCDZ6ZWeeagZOB3ydPvK4E1gPzIuInFB6U+PdkuZ8Bb4uIXRQaug8CW4DvAp+MiEfb20HyipSzgEnAxmSdHwDDypjLzDJKEdH5UmZmZmbWZ/gMnpmZmVnGuMEzMzMzyxg3eGZmZmYZ4wbPzMzMLGP8Hrwiw4cPj6OOOirtMlLx6quvMnjw4LTL6HF5zQ35zZ7X3ODsecye19yQj+yrV6/eEhGj2pvnBq/I6NGjWbVqVdplpKK+vp7a2tq0y+hxec0N+c2e19zg7HnMntfckI/skp7uaJ4v0ZqZmZlljBs8MzMzs4xxg2dmZmaWMb4Hz8zMLKfeeOMNNm3axM6dO9MupdsNGzaMDRs2pF1Gt6isrOSwww6joqKiy+u4wTMzM8upTZs2UV1dzbhx45CUdjndqrm5merq6rTLKFlEsHXrVjZt2sSRRx7Z5fX8W7RFjhh/VPSbfW3aZaRi3sTdLGzMX7+f19yQ3+x5zQ3OnsfsneW+/iPvYPTh47u1uTv+sOHdtq1SZKXBg0KT9+ijj3Lssce+abqk1RExpb11fA+emZlZTgll7sxdFh3IMXKDZ2ZmZpYx+TtfbWZmZu36yHXLunV7TQtmdbrMc889x6WXXspDDz3E8OHDGT16NIsWLeKYY47p8n4WLVrE3LlzOeigg0opl/vuu48rrrgCgCeeeIIxY8ZQVVXF8ccfzy233FLStntarzmDJ6lFUkPR51NFw7skNSbDC9qs93ZJSyVtl3Rdm3n1kh4r2s7BPZvKzMzMOhIRnH322dTW1vLkk0+yevVqrr76ap5//vn92s6iRYt47bXXSq7nzDPPpKGhgYaGBqZMmcLtt99OQ0NDjzd3LS0tJW+j1zR4wI6ImFT0ubF1GNgM1CXjV7ZZbyfwf4AvdrDdC4q2+UI5A5iZmVnXLV26lIqKCi6++OK900444QROPPFEZsyYwUknncTEiRP5+c9/DhR+X3bWrFmccMIJHHfccSxZsoRvf/vbbN68mbq6Ourq6gBYvHgxp5xyCscdd9zeM3IAQ4YM4fLLL6empob3v//9/OEPf6C2tpbx48dz1113dVjnZz/7WaZMmUJNTQ1XXXXV3un33HMPEyZMYPLkyVxyySWcddZZALz44ot84AMfoKamhk9/+tOMHTuWLVu2AHDbbbcxdepUJk2axEUXXbS3mRsyZAjz5s3jhBNOYMWKFSX/3famBu+ARMSrEfGfFBo9MzMz6yPWr1/P5MmT3zK9srKSO++8kzVr1rB06VLmzZtHRPCrX/2KQw89lLVr17J+/XpmzpzJJZdcwqGHHsrSpUtZunQpmzdv5oorruCXv/wlDQ0NPPTQQ/zsZz8DCg3i6aefziOPPEJ1dTVf/vKX+fWvf82dd97J/PnzO6zzG9/4BqtWrWLdunU88MADrFu3jp07d3LRRRdx7733snr1al588cW9y3/1q1/du5+Pf/zj/PnPfwZgw4YNLFmyhGXLltHQ0ED//v25/fbb99Z28skns3btWt7znveU/Hfbm+7Bq5LUkAxvjIizu2m7N0pqAX4KfD3avBdG0lxgLsDIkaOYP3F3N+22bxldVXicPm/ymhvymz2vucHZ85i9s9xDBwajq8q3/+bm5n3O37lzJ7t27XrLcm+88QZXXnkly5cvp1+/fjz77LM8+eSTHHnkkdx///1cdtllzJw5k2nTptHc3ExEsH37dgYNGsSDDz7I9OnTGTFiBDt27OCcc87hN7/5DTNmzGDgwIFMnz6d5uZmjjnmGAYNGsTOnTsZN24cTU1Nb6qjpaWFV199lebmZm655RZuuukmdu/ezXPPPcfq1atpbm5m7NixjBw5kubmZj72sY9x44030tzczIMPPsjtt99Oc3Mz06dPZ/jw4Wzfvp27776bVatW7W1qd+zYwbBhw2hubqZ///6cccYZHf6d7dy5k/r6+i7/3femBm9Hcjm2O10QEc9KqqbQ4P0t8KYL6RFxPXA9FN6Dl8f3JIHfEZVHec2e19zg7HnM3lnuG44Uz+8o3/47ew/d5MmT+eUvf/mW5W666SZefvllHn74YSoqKhg3bhwDBgzgpJNO4uGHH+aee+7hX/7lX5gxYwbz589HEkOGDKG6upqqqioqKiro378/1dXVVFZWMnDgQKqrq6moqGDo0KEAVFVV7V0HYPfu3W+qo3///gwePJgtW7Zw3XXX8dBDDzFixAjmzJmDJAYPHrx3H63bGzBgANXV1fTr1+9N226tb9CgQcyZM4err776LX8XlZWVDB/e8TsEKysrOfHEE7vwt17Q5y/R7ktEPJv82Qz8OzA13YrMzMys1emnn87rr7/O9ddfv3faunXrePrppzn44IOpqKhg6dKlPP300wBs3ryZgw46iE984hNcfvnlrFmzBig0kq1nvqZOncoDDzzA1q1baWlpYfHixZx22mkHXOMrr7zC4MGDGTZsGM8//zz33nsvAO985zt56qmnaGpqAmDJkiV715k+fTo//vGPAbj//vt56aWXAJgxYwZ33HEHL7xQeCRg27Zte7N1t8z+74ykAcDwiNgiqQI4C/hNymWZmZn1Wnf94/SSt7E/v2QhiTvvvJNLL72Ub37zm1RWVjJu3Di+8pWvcMkllzBx4kSmTJnChAkTAGhsbOTyyy+nX79+VFRU8L3vfQ+AuXPnMnPmzL334i1YsIBZs2YhiVmzZvHRj370gPO0PvQxYcIEDj/8cKZPL/wdVVVV8d3vfpeZM2cyePBg3v3ud+9d56qrruL888/n1ltv5dRTT+Ud73gH1dXVjBw5kq9//eucccYZ7Nmzh4qKCr7zne8wduzYA66vI5lo8CQ1AUOBgZI+BpwBPA3clzR3/Sk0dzekVqSZmZm9xaGHHrr3bFex9p4kHTduHGeeeeZbpn/+85/n85///N7x888/n7POOustl363b9++d/grX/lKh/OAN93vdtNNN7Vbe11dHY8++igRwec+9zmmTCn8atiwYcO47777GDBgACtWrOChhx5i0KBBAJx77rmce+65b9lW2/2Xqtc0eBExZB/zxnWybkfz3/pojpmZmVk3uOGGG7j55pvZtWsXJ554IhdddBEAf/7zn5k9ezZ79uxh4MCB3HBDz59f6jUNnpmZmVlfctlll3HZZZe9ZfrRRx/Nww8/nEJF/80NXpGqiv481oWfVcmi+vp6mi6oTbuMHpfX3JDf7HnNDc6ex+yd5d6wYQMTxgw7oB+zt57T5g1vXZLpp2jNzMysY5WVlWzduvWAGgjrGRHB1q1bqays3K/1fAbPzMwspw477DA2bdr0pl9hyIqdO3fud1PUW1VWVnLYYYft1zpu8MzMzHKqoqKCI488Mu0yyqK+vn6/XgycNb5Ea2ZmZpYxbvDMzMzMMsYNnpmZmVnGuMEzMzMzyxg3eGZmZmYZ4wbPzMzMLGPc4JmZmZlljBs8MzMzs4xxg2dmZmaWMfLvz/23I8YfFf1mX5t2GamYN3E3Cxvz98Mmec0N+c2e19zg7HnMntfckG72pgWzemQ/klZHxJT25vkMnpmZmVnGuMEzMzMzyxg3eGZmZmYZU5YGT9I1ki4tGr9P0g+KxhdK+oKkSZJWSHpE0jpJ5ybzr5J0dZttTpK0oZ19/aOkJySFpJFF02slvSypIfnML0dWMzMzs96mXGfwlgHTACT1A0YCNUXzpwHLgdeAT0ZEDTATWCRpOLAYOLfNNs9Lpre3r/cDT7cz73cRMSn5fK2EPGZmZmZ9RrkavOXAqclwDbAeaJY0QtIg4FhgTUQ8HhF/AoiIzcALwKiIeBx4SdLJRducTTsNXkQ8HBFNZcphZmZm1ueU7TUpkjYCpwEfBASMAVYALwMLIuK9bZafCtwM1ETEHklfBMZExGWSTgGu6+hR4GT9JmBKRGxJxmuBnwKbgM3AFyPikXbWmwvMBRg5ctTk+YtuKCl3XzW6Cp7fkXYVPS+vuSG/2fOaG5w9j9nzmhvSzT5xzLAe2U9dXV2Hr0kp5wtillO4FDsN+BaFBm8ahQZvWfGCkg4BbgUujIg9yeQlwHJJ8+j48uy+rAHGRsR2SR8CfgYc3XahiLgeuB4K78Hz+4LyJa+5Ib/Z85obnD2P2fOaG1J+D94Ftanst1g5n6JtvQ9vIoVLtCspXLZtvf8OAElDgbuBL0XEytbpEfEM0HoW8BwKDV+XRcQrEbE9Gb4HqCh+CMPMzMwsq8rZ4C0HzgK2RURLRGwDhlNo8pYDSBoI3AncEhF3tLONxcA1wFMRsWl/di7pHZKUDE+lkHXrgYYxMzMz6yvK2eA1Unh6dmWbaS+33idH4cGJ9wFzil5nMqlo+Z9QeEijw8uzki6RtAk4DFhX9DqWjwPrJa0Fvg2cF/5dNjMzM8uBsl2cjogWYGibaXPajN8G3LaPbWwBKjrZz7cpNHBtp18HXNf1is3MzMyywb9kYWZmZpYx+Xy0pgNVFf15bMGstMtIRX19fa946qen5TU35Dd7XnODs+cxe15zQ76zg8/gmZmZmWWOGzwzMzOzjHGDZ2ZmZpYxbvDMzMzMMsYNnpmZmVnGuMEzMzMzyxg3eGZmZmYZ4wbPzMzMLGPc4JmZmZlljBs8MzMzs4xxg2dmZmaWMW7wzMzMzDLGDZ6ZmZlZxigi0q6h1zhi/FHRb/a1aZeRinkTd7OwcUDaZfS4vOaG/GbPa25w9jxmL3fupgWzyrbtUtXX11NbW5t2GWUlaXVETGlvns/gmZmZmWWMGzwzMzOzjOn1DZ6kFkkNRZ9PFQ3vktSYDC9os95ASTcm89dKqk0pgpmZmVmP6gs3JOyIiEltpt0IIKkJqIuILe2s9xmAiJgo6WDgXknvjog9Za3WzMzMLGW9/gxeCd4F/BYgIl4A/gK0eyOimZmZWZb0+qdoJbUAjcnoxog4u2heEzClvTN4kuYCHwDOBw4HHgb+PiJ+2s5ycwFGjhw1ef6iG8oRo9cbXQXP70i7ip6X19yQ3+x5zQ3Onsfs5c49ccyw8m28RNu3b2fIkCFpl1FWdXV1HT5F21cv0XbFD4FjgVXA08ByoKXtQhFxPXA9FF6TksfH6MGvEMijvGbPa25w9jxmL/trUi6oLdu2S5WH16TsS2b/tUfEbuCy1nFJy4HH06vIzMzMrGdk9h48SQdJGpwMfwDYHRF/TLksMzMzs7LL7Bk84GDgPkl7gGeBv025HjMzM7Me0esbvIjo8A7JiBi3j3lNwDvLUJKZmZlZr5bZS7RmZmZmedXrz+D1pKqK/jzWi384uZzq6+t79dNQ5ZLX3JDf7HnNDc6ex+x5zW0+g2dmZmaWOW7wzMzMzDLGDZ6ZmZlZxrjBMzMzM8sYN3hmZmZmGeMGz8zMzCxj3OCZmZmZZYwbPDMzM7OMcYNnZmZmljFu8MzMzMwyxg2emZmZWcbss8GT1F/S7T1VjJmZmZmVbp8NXkS0AGMlDeyheszMzMysRIqIfS8g3QIcC9wFvNo6PSK+Vd7Set4R44+KfrOvTbuMVMybuJuFjQPSLqPH5TU35Dd7XnODs+cxe15zQ3mzNy2YVZbt7i9JqyNiSnvzupL8yeTTD6juzsLMzMzMrPt12uBFxFcBJB0UEa+VvyQzMzMzK0WnT9FKOlXSH4FHk/ETJH33QHco6RpJlxaN3yfpB0XjCyV9QdIkSSskPSJpnaRzk/lXSbq6zTYnSdrQzr5Ol7RG0npJN0vK53lqMzMzy5WuvCZlEXAmsBUgItYC7ythn8uAaQCS+gEjgZqi+dOA5cBrwCcjogaYCSySNBxYDJzbZpvnJdP3SrZ9M3BeRBwHPA1cWELdZmZmZn1Cl96DFxHPtJnUUsI+lwOnJsM1wHqgWdIISYMoPNCxJiIej4g/JfvfDLwAjIqIx4GXJJ1ctM3ZtGnwgLcDu5LlAX4NnFNC3WZmZmZ9QlcuWT4jaRoQkiqA/wm85XJoV0XEZkm7JR1B4WzdCmAMhabvZaAxInYVryNpKjCQwsMeUGjmzgN+L+kUYFtrM1hkCzBA0pSIWAV8HDi8bT2S5gJzAUaOHMX8ibsPNFqfNrqq8MRR3uQ1N+Q3e15zg7PnMXtec0N5s9fX15dlu92pKw3excC1FJqwZ4H7gc+VuN/lFJq7acC3km1Po9DgLSteUNIhwK3AhRGxJ5m8BFguaR7tXJ4FiIiQdB5wTXJm8H7aOfMYEdcD10PhNSl+nDxf8pob8ps9r7nB2fOYPa+5ocyvSbmgtizb7U5dSV4VERcUT5D0jhL323of3kQKl2ifAeYBrwA3Fu1nKHA38KWIWNk6PSKekbQROI3CZddTaUdErADem2zrDOCYEus2MzMz6/W6cg/eRkmLJVUVTbunxP0uB86icGm1JSK2AcMpNGrLAZJfz7gTuCUi7mhnG4uBa4CnImJTezuRdHDy5yDgCuD7JdZtZmZm1ut1pcFrBH4HLJP0V8k0lbjfRgpPz65sM+3liNiSjM+m8LTuHEkNyWdS0fI/ofCQxlsuzxa5PHl9yjrgFxHx2xLrNjMzM+v1unKJNiLiu5LWAr+QdAWw798363yDLcDQNtPmtBm/DbhtH9vYAlR0sp/LgcsPuFAzMzOzPqgrDZ4AImKZpBnAj4EJZa3KzMzMzA5YVxq8D7UORMR/SaojeVFx1lRV9OexXvIDwj2tvr6+TzwV1N3ymhvymz2vucHZ85g9r7kh39lhHw2epE8kl0nPl9q95e7BslVlZmZmZgdsX2fwBid/VvdEIWZmZmbWPTps8CLi3yT1B16JiGt6sCYzMzMzK8E+X5OSPO16fg/VYmZmZmbdoCsPWSyTdB2Fnwd7tXViRKwpW1VmZmZmdsC60uC1vlz4a0XTAji9+8sxMzMzs1J12uBFRF1PFGJmZmZm3aPTnyqTNEzStyStSj4LJQ3rieLMzMzMbP915bdofwg0U/ht2NnAK8CN5SzKzMzMzA5cV+7B+6uIOKdo/KuSGspVkJmZmZmVpitn8HZIek/riKTpwI7ylWRmZmZmpejKGbyLgVuK7rt7CbiwfCWZmZmZWSm60uC9EhEnSBoKEBGvSDqyzHWZmZmZ2QFSROx7AWlNRJzUZtrqiJhc1spScMT4o6Lf7GvTLiMV8ybuZmFjV/r9bMlrbshv9rzmBmfPY/buzt20YFa3bavc6uvrqa2tTbuMskr6sSntzevwqEuaANQAwyT9j6JZQ4HK7i3RzMzMzLrLvtr6dwJnAcOBDxdNbwY+U86izMzMzOzAddjgRcTPgZ9LOjUiVvRgTWZmZmZWgq68JuViScNbRySNkPTDMtb0JpJaJDUUfT5VNLxLUmMyvKDNeuMk7Sha9vs9VbOZmZlZmrpy5+XxEfGX1pGIeEnSiWWsqa0dETGpzbQbASQ1AXURsaWDdZ9sZ10zMzOzTOvKGbx+kka0jkh6G11rDM3MzMwsBV15Tcongf8N/CSZ9NfANyLi1jLX1rr/FqAxGd0YEWcXzWsCprR3Bk/SOOAR4HEKv5/75Yj4XTvLzQXmAowcOWry/EU3dHOCvmF0FTyfw98nyWtuyG/2vOYGZ89j9u7OPXHMsM4X6iW2b9/OkCFD0i6jrOrq6jp8TUqnDR6ApHcBpyejv42IP3ZjfZ3te3tEtHuEOmnwBgFDImKrpMnAz4CaiHilo335PXj5OzGb19yQ3+x5zQ3Onsfsfg9ebdpllNW+3oPXlUu0AG8DXo2I64AX+8IvWUTE6xGxNRleDTwJHJNuVWZmZmbl12mDJ+kq4Argn5JJFcBt5SyqO0gaJal/MjweOBp4Kt2qzMzMzMqvK+dtzwZOBNYARMRmSdVlrap7vA/4mqQ3gD3AxRGxLeWazMzMzMquKw3erogISQEgaXCZa3qTju6/S+aN28e8nwI/LUdNZmZmZr1ZVxq8H0v6N2C4pM8Afwdk8lHTqor+PNaHbiDtTvX19TRdUJt2GT0ur7khv9nzmhucPY/Z85rbutbgBfCfFF41cgwwPyJ+XdaqzMzMzOyAdaXBG0LhrN02YAmwrqwVmZmZmVlJOn2KNiK+GhE1wOeAQ4AHJP2m7JWZmZmZ2QHp6nvwAF4AngO2AgeXpxwzMzMzK1VX3oP3D5Lqgf8A3g58JiKOL3dhZmZmZnZgunIP3uHApRHRUO5izMzMzKx0nTZ4EfFPnS1jZmZmZr3H/tyDZ2ZmZmZ9gBs8MzMzs4xxg2dmZmaWMW7wzMzMzDLGDZ6ZmZlZxrjBMzMzM8sYRUTaNfQaR4w/KvrNvjbtMlIxb+JuFjZ25bWI2ZLX3JDf7HnNDc6ex+x5zQ2lZ29aMKsbqykPSasjYkp783wGz8zMzCxj3OCZmZmZZYwbPDMzM7OM6fEGT9I1ki4tGr9P0g+KxhdK+oKkSZJWSHpE0jpJ5ybzr5J0dZttTpK0oZ193SRpo6SG5DOpnNnMzMzMeoM0zuAtA6YBSOoHjARqiuZPA5YDrwGfjIgaYCawSNJwYDFwbpttnpdMb8/lETEp+TR0XwwzMzOz3imNBm85cGoyXAOsB5oljZA0CDgWWBMRj0fEnwAiYjPwAjAqIh4HXpJ0ctE2Z9Nxg2dmZmaWK6m8JkXSRuA04IOAgDHACuBlYEFEvLfN8lOBm4GaiNgj6YvAmIi4TNIpwHXtPSYs6SYKzeTrwH8AV0bE622WmQvMBRg5ctTk+Ytu6NasfcXoKnh+R9pV9Ly85ob8Zs9rbnD2PGbPa24oPfvEMcO6r5gyqaur6/A1KWk1eLcDv6DQ4H2LQoM3jUKD9/aIuLJo2UOAeuDCiFiZTDucwpnAscn6z0TEwnb2cwjwHDAQuB54MiK+1lFdfg9e/t6VlNfckN/sec0Nzp7H7HnNDX4PXlpP0bbehzeRwiXalRTOtLXefweApKHA3cCXWps7gIh4Bmg9C3gOsKS9nUTEf0XB68CNwNSypDEzMzPrRdJq8JYDZwHbIqIlIrYBwyk0ecsBJA0E7gRuiYg72tnGYuAa4KmI2NTeTpIzeEgS8DEKzaSZmZlZpqXV4DVSeHp2ZZtpL0fElmR8NvA+YE4Hrzn5CYWHNPb1cMXtkhqL9vf17gpgZmZm1lulcmE+IlqAoW2mzWkzfhtw2z62sQWo6GQ/px94lWZmZmZ9k3/JwszMzCxj8vloTQeqKvrzWB94aqYc6uvrabqgNu0yelxec0N+s+c1Nzh7HrPnNTfkOzv4DJ6ZmZlZ5rjBMzMzM8sYN3hmZmZmGeMGz8zMzCxj3OCZmZmZZYwbPDMzM7OMcYNnZmZmljFu8MzMzMwyxg2emZmZWca4wTMzMzPLGDd4ZmZmZhnjBs/MzMwsYxQRadfQaxwx/qjoN/vatMtIxbyJu1nYOCDtMnpcXnNDfrPnNTc4ex6zd1fupgWzuqGanlVfX09tbW3aZZSVpNURMaW9eT6DZ2ZmZpYxbvDMzMzMMsYNnpmZmVnG9PoGT1KLpIaiz6eKhndJakyGF7RZb2rRcmslnZ1WBjMzM7Oe1BfuON0REZPaTLsRQFITUBcRW9pZbz0wJSJ2SzoEWCvpFxGxu7zlmpmZmaWrLzR4ByQiXisarQT8uLCZmZnlQq9/TYqkFqAxGd0YEWcXzWuicJauvTN4SDoZ+CEwFvjbiLiznWXmAnMBRo4cNXn+ohu6N0AfMboKnt+RdhU9L6+5Ib/Z85obnD2P2bsr98Qxw0rfSA/bvn07Q4YMSbuMsqqrq+vwNSl9ocHbHhHtHqHOGryi5Y4FbgbeFxE7O1rO78HL7AndDuU1N+Q3e15zg7PnMbvfg1ebdhlllfv34EXEBmA7cFzatZiZmZmVW2YbPElHShqQDI8FJgBNqRZlZmZm1gOyfL76PcCVkt4A9gD/0NmlXDMzM7Ms6PUNXkf33yXzxu1j3q3AreWoyczMzKw3y+wlWjMzM7O86vVn8HpSVUV/HuuDTwp1h/r6epouqE27jB6X19yQ3+x5zQ3Onsfsec1tPoNnZmZmljlu8MzMzMwyxg2emZmZWca4wTMzMzPLGDd4ZmZmZhnjBs/MzMwsY9zgmZmZmWWMGzwzMzOzjHGDZ2ZmZpYxbvDMzMzMMsYNnpmZmVnGuMEzMzMzyxhFRFh95wYAAAfySURBVNo19BpHjD8q+s2+Nu0yUjFv4m4WNg5Iu4wel9fckN/sec0Nzp7H7HnNDd2TvWnBrG6qpjwkrY6IKe3N8xk8MzMzs4xxg2dmZmaWMW7wzMzMzDKmxxs8SddIurRo/D5JPygaXyjpC5ImSVoh6RFJ6ySdm8y/StLVbbY5SdKGdvb1/yStTda/Q9KQcmYzMzMz6w3SOIO3DJgGIKkfMBKoKZo/DVgOvAZ8MiJqgJnAIknDgcXAuW22eV4yva3LIuKEiDge+DPwj90ZxMzMzKw3SqPBWw6cmgzXAOuBZkkjJA0CjgXWRMTjEfEngIjYDLwAjIqIx4GXJJ1ctM3ZtNPgRcQrAJIEVAF+ZNjMzMwyL5XXpEjaCJwGfBAQMAZYAbwMLIiI97ZZfipwM1ATEXskfREYExGXSToFuK6jx4Ql3Qh8CPgjMCsiXmszfy4wF2DkyFGT5y+6oRuT9h2jq+D5HWlX0fPymhvymz2vucHZ85g9r7mhe7JPHDOse4opk7q6ug5fk5JWg3c78AsKDd63KDR40yg0eG+PiCuLlj0EqAcujIiVybTDKZwJHJus/0xELNzH/voD/xd4KCJu7Gg5vwcvf+9KymtuyG/2vOYGZ89j9rzmBr8HL62naFvvw5tI4RLtSgqXbVvvvwNA0lDgbuBLrc0dQEQ8A7SeBTwHWLKvnUVEC/CjZFkzMzOzTEurwVsOnAVsi4iWiNgGDKfQ5C0HkDQQuBO4JSLuaGcbi4FrgKciYlPbmSo4qnUY+AjwaDnCmJmZmfUmaTV4jRSenl3ZZtrLEbElGZ8NvA+YI6kh+UwqWv4nFB7SaO/pWSjc23ezpMZk24cAX+vGDGZmZma9UioX5pNLpkPbTJvTZvw24LZ9bGMLULGP+XuA6SUVamZmZtYH+ZcszMzMzDImn4/WdKCqoj+P9fInZsqlvr6epgtq0y6jx+U1N+Q3e15zg7PnMXtec0O+s4PP4JmZmZlljhs8MzMzs4xxg2dmZmaWMW7wzMzMzDLGDZ6ZmZlZxrjBMzMzM8sYRUTaNfQakpqBx9KuIyUjgS2dLpU9ec0N+c2e19zg7HnMntfckI/sYyNiVHsz/B68N3ssIqakXUQaJK3KY/a85ob8Zs9rbnD2PGbPa27Id3bwJVozMzOzzHGDZ2ZmZpYxbvDe7Pq0C0hRXrPnNTfkN3tec4Oz51Fec0O+s/shCzMzM7Os8Rk8MzMzs4xxg2dmZmaWMblp8CTNlPSYpCckXdnO/EGSliTzfy9pXNG8f0qmPybpzJ6su1QHmlvSOEk7JDUkn+/3dO2l6kL290laI2m3pI+3mXehpD8lnwt7rurSlZi7peiY39VzVXePLmT/gqQ/Slon6T8kjS2al+Vjvq/cWT/mF0tqTPL9p6R3Fc3rs9/tcODZ+/r3e2e5i5Y7R1JImlI0rU8f8/0SEZn/AP2BJ4HxwEBgLfCuNsv8A/D9ZPg8YEky/K5k+UHAkcl2+qedqQdyjwPWp52hzNnHAccDtwAfL5r+NuCp5M8RyfCItDOVO3cyb3vaGcqcvQ44KBn+bNG/96wf83Zz5+SYDy0a/gjwq2S4z363d0P2Pvv93pXcyXLVwIPASmBKFo75/n7ycgZvKvBERDwVEbuAHwEfbbPMR4Gbk+E7gBmSlEz/UUS8HhEbgSeS7fUFpeTu6zrNHhFNEbEO2NNm3TOBX0fEtoh4Cfg1MLMniu4GpeTu67qSfWlEvJaMrgQOS4azfsw7yt3XdSX7K0Wjg4HWJwv78nc7lJa9L+vKf9cA/hn4JrCzaFpfP+b7JS8N3hjgmaLxTcm0dpeJiN3Ay8Dbu7hub1VKboAjJT0s6QFJ7y13sd2slOOW9WO+L5WSVklaKelj3Vta2e1v9r8H7j3AdXuTUnJDDo65pM9JehL4V+CS/Vm3FyslO/Td7/dOc0s6CTg8Iu7e33WzxD9VZh35L+CIiNgqaTLwM0k1bf6P0LJnbEQ8K2k88FtJjRHxZNpFdTdJnwCmAKelXUtP6iB35o95RHwH+I6kvwG+DPSpeyxL0UH2zH6/S+oHfAuYk3IpqcvLGbxngcOLxg9LprW7jKQBwDBgaxfX7a0OOHdyCnsrQESspnCvwjFlr7j7lHLcsn7MOxQRzyZ/PgXUAyd2Z3Fl1qXskt4PfAn4SES8vj/r9lKl5M7FMS/yI6D1LGVfPuZQQvY+/v3eWe5q4DigXlITcApwV/KgRV8/5vsn7ZsAe+JD4UzlUxRuqmy9KbOmzTKf480PG/w4Ga7hzTdlPkUfuSmzxNyjWnNSuJn1WeBtaWfqzuxFy97EWx+y2EjhZvsRyXCfyF5i7hHAoGR4JPAn2rl5ubd+uvjv/UQK/zE7us30TB/zfeTOwzE/umj4w8CqZLjPfrd3Q/Y++/2+P99xyfL1/PdDFn36mO/331XaBfTgP4oPAY8nX3JfSqZ9jcL/zQJUAj+hcNPlH4DxRet+KVnvMeCDaWfpidzAOcAjQAOwBvhw2lnKkP3dFO7BeJXC2dpHitb9u+Tv5AngU2ln6YncwDSgMfkCbAT+Pu0sZcj+G+D55N91A3BXTo55u7lzcsyvLfouW0pRM9CXv9tLyd7Xv987y91m2XqSBi8Lx3x/Pv6pMjMzM7OMycs9eGZmZma54QbPzMzMLGPc4JmZmZlljBs8MzMzs4xxg2dmZmaWMW7wzMzMzDLGDZ6ZmZlZxvx/CxbsrVwRAcoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxCBcj9Bwnsw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgAil7OYSAoP"
      },
      "source": [
        "5. сравнить все реализованные методы сделать выводы\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKdQUAou0Bpa"
      },
      "source": [
        "Лучше всех показал себя Countvectorizer по буквам. Результаты с эмбеддингами сравнительно похожи на лучшие результаты UnigramTagger и его комбинации, но уступают. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cINqgGpKXURp"
      },
      "source": [
        "# Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCM0drjKXYet"
      },
      "source": [
        "много дополнительных датасетов на русском языке\n",
        "\n",
        "https://natasha.github.io/corus/  \n",
        "https://github.com/natasha/corus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUOg4C8sZNpw"
      },
      "source": [
        "мы будем использовать данные http://ai-center.botik.ru/Airec/index.php/ru/collections/28-persons-1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzi6ApNLZg6X"
      },
      "source": [
        "**Проверить насколько хорошо работает NER**\n",
        "\n",
        "1. взять нер из nltk\n",
        "2. проверить deeppavlov\n",
        "3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них\n",
        "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1LgaNUtaOz"
      },
      "source": [
        "при обучении своего нера незабудьте разделить выборку"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg6tcss2Zhp9",
        "outputId": "15450aca-e114-49f4-aca8-1eafaca2d033"
      },
      "source": [
        "!pip install corus"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting corus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/a3/e680679c669b0118271ac7246549c75a7088ab0e6696a3561408a3f9b50d/corus-0.9.0-py3-none-any.whl (83kB)\n",
            "\r\u001b[K     |████                            | 10kB 10.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 3.2MB/s \n",
            "\u001b[?25hInstalling collected packages: corus\n",
            "Successfully installed corus-0.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrc5ocDkaS1e"
      },
      "source": [
        "import corus"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPG7VIZJbH76",
        "outputId": "af234b09-3340-44d9-ba8b-ca4bd272a282"
      },
      "source": [
        "!wget http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-12 20:42:19--  http://ai-center.botik.ru/Airec/ai-resources/Persons-1000.zip\n",
            "Resolving ai-center.botik.ru (ai-center.botik.ru)... 95.129.138.2\n",
            "Connecting to ai-center.botik.ru (ai-center.botik.ru)|95.129.138.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3363777 (3.2M) [application/zip]\n",
            "Saving to: ‘Persons-1000.zip’\n",
            "\n",
            "Persons-1000.zip    100%[===================>]   3.21M  2.75MB/s    in 1.2s    \n",
            "\n",
            "2021-04-12 20:42:21 (2.75 MB/s) - ‘Persons-1000.zip’ saved [3363777/3363777]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmGWX23KbUfI",
        "outputId": "b0db0f65-b083-46af-9e0e-a7d448ebde57"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets  Persons-1000.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmJr9tubbTVk"
      },
      "source": [
        "path = 'Persons-1000.zip'\n",
        "records = corus.persons.load_persons(path)\n",
        "rec = next(records)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzQutiCephkx",
        "outputId": "42d40db1-493a-4f1a-8904-f21e05e448af"
      },
      "source": [
        "rec"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PersonsMarkup(\n",
              "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
              "    spans=[PersonsSpan(\n",
              "         id=1,\n",
              "         start=308,\n",
              "         stop=324,\n",
              "         value='ГРИГОРИЙ КАРАСИН'\n",
              "     ), PersonsSpan(\n",
              "         id=2,\n",
              "         start=387,\n",
              "         stop=402,\n",
              "         value='ДЭНИЭЛ ФРИД'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utanZXahn-2I",
        "outputId": "36e8b00d-5eb5-4bf1-cb67-1001db18175b"
      },
      "source": [
        "print(rec.text)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Россия рассчитывает на конструктивное воздействие США на Грузию\r\n",
            "\r\n",
            "04/08/2008 12:08\r\n",
            "\r\n",
            "МОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\r\n",
            "\r\n",
            "\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\r\n",
            "\r\n",
            "\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBaxNLbQoucW",
        "outputId": "add74f35-e876-43a1-8f89-d05103bbc0c1"
      },
      "source": [
        "rec.spans"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PersonsSpan(\n",
              "     id=1,\n",
              "     start=308,\n",
              "     stop=324,\n",
              "     value='ГРИГОРИЙ КАРАСИН'\n",
              " ), PersonsSpan(\n",
              "     id=2,\n",
              "     start=387,\n",
              "     stop=402,\n",
              "     value='ДЭНИЭЛ ФРИД'\n",
              " )]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku7z5W_mnpFF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oRyOTnGtVSw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7ec094-9a61-4669-bbf5-8b8df5f1645f"
      },
      "source": [
        "!pip install razdel"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting razdel\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv4GhkdztVNC"
      },
      "source": [
        "from razdel import tokenize"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogf5-MIR9Dnu"
      },
      "source": [
        "records = corus.persons.load_persons(path)\n",
        "words_docs = []\n",
        "for ix, rec in enumerate(records):\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        is_person = False\n",
        "        for person in rec.spans:\n",
        "            if (token.start >= person.start) and (token.stop <= person.stop):\n",
        "                is_person = True\n",
        "                break\n",
        "        words.append([token.text, 'PERSON' if is_person else 'NO_PERSON'])\n",
        "    words_docs.extend(words)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELOiCSmY-rfj"
      },
      "source": [
        "теперь у нас есть слово и тег персона или нет, уже можно что-то делать(пробовать какие-то мл алгоритмы)  \n",
        "*дополнительно попробуйте проанализировать все ли токены если рассматривать униграмный подход относятся к тегам персона с логической точки зрения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMg8h9tO-kV0",
        "outputId": "1589b5b0-5d1e-43ba-dac2-9be6c275b444"
      },
      "source": [
        "words_docs[:5]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Россия', 'NO_PERSON'],\n",
              " ['рассчитывает', 'NO_PERSON'],\n",
              " ['на', 'NO_PERSON'],\n",
              " ['конструктивное', 'NO_PERSON'],\n",
              " ['воздействие', 'NO_PERSON']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLTxpPMLDuun"
      },
      "source": [
        "1. взять нер из nltk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xtam1-3-kMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3ed1b2-6daf-492e-a534-ac3bfa8cbd79"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyRlJsxpEGlU",
        "outputId": "e997ff69-a809-48c9-ae32-7bb0f69f2672"
      },
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(rec.text))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Мать', 'JJ'),\n",
              " ('рядового', 'NNP'),\n",
              " ('Владислава', 'NNP'),\n",
              " ('Челаха', 'NNP'),\n",
              " (',', ','),\n",
              " ('обвиняемого', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('убийстве', 'VBD'),\n",
              " ('14', 'CD'),\n",
              " ('пограничников', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('егеря', 'NNP'),\n",
              " (',', ','),\n",
              " ('после', 'NNP'),\n",
              " ('свидания', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('сыном', 'NNP'),\n",
              " ('заметила', 'NNP'),\n",
              " ('следы', 'NNP'),\n",
              " ('побоев', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Как', 'VB'),\n",
              " ('передаёт', 'JJ'),\n",
              " ('РИА', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Новости', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('со', 'CC'),\n",
              " ('ссылкой', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('местную', 'NNP'),\n",
              " ('газету', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Время', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('Владислав', 'NNP'),\n",
              " ('Челах', 'NNP'),\n",
              " (',', ','),\n",
              " ('несший', 'NNP'),\n",
              " ('службу', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('пограничной', 'NNP'),\n",
              " ('заставе', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Арканкерген', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('в', 'CC'),\n",
              " ('Алма-Атинской', 'JJ'),\n",
              " ('области', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('границе', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('Китаем', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('подозреваемый', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('убийстве', 'VBD'),\n",
              " ('14', 'CD'),\n",
              " ('сослуживцев', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('егеря', 'NNP'),\n",
              " (',', ','),\n",
              " ('заявил', 'NNP'),\n",
              " ('своей', 'NNP'),\n",
              " ('матери', 'NNP'),\n",
              " ('во', 'NNP'),\n",
              " ('время', 'NNP'),\n",
              " ('свидания', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('никого', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('убивал', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Сама', 'VB'),\n",
              " ('женщина', 'JJ'),\n",
              " ('заявила', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('встречаться', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('сыном', 'NNP'),\n",
              " ('ей', 'NNP'),\n",
              " ('пришлось', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('присутствии', 'NNP'),\n",
              " ('видеокамеры', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Ей', 'VB'),\n",
              " ('даже', 'JJ'),\n",
              " ('пришлось', 'NNP'),\n",
              " ('подписать', 'NNP'),\n",
              " ('согласие', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('это', 'NNP'),\n",
              " (',', ','),\n",
              " ('иначе', 'NNP'),\n",
              " ('бы', 'NNP'),\n",
              " ('свидание', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('состоялось', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('``', '``'),\n",
              " ('Меня', 'JJ'),\n",
              " ('завели', 'NN'),\n",
              " ('в', 'NNP'),\n",
              " ('маленькую', 'NNP'),\n",
              " ('комнатушку', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Следом', 'VB'),\n",
              " ('залетели', 'JJ'),\n",
              " ('несколько', 'NNP'),\n",
              " ('человек', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('видеокамерой', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Они', 'VB'),\n",
              " ('снимали', 'JJ'),\n",
              " ('нас', 'NNP'),\n",
              " ('все', 'NNP'),\n",
              " ('эти', 'VBD'),\n",
              " ('5', 'CD'),\n",
              " ('минут', 'NN'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('длилось', 'NNP'),\n",
              " ('свидание', 'NNP'),\n",
              " (',', ','),\n",
              " ('чтобы', 'NNP'),\n",
              " ('Влад', 'NNP'),\n",
              " ('ничего', 'NNP'),\n",
              " ('лишнего', 'NNP'),\n",
              " ('мне', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('сказал', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Я', 'NN'),\n",
              " ('расплакалась', 'NN'),\n",
              " (',', ','),\n",
              " ('подошла', 'NNP'),\n",
              " ('к', 'NNP'),\n",
              " ('сыну', 'NNP'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('обнял', 'NNP'),\n",
              " ('меня', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('шепнул', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('ухо', 'NN'),\n",
              " (':', ':'),\n",
              " ('``', '``'),\n",
              " ('Мам', 'NN'),\n",
              " (',', ','),\n",
              " ('я', 'NNP'),\n",
              " ('этого', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('делал', 'NN'),\n",
              " ('!', '.'),\n",
              " (\"''\", \"''\"),\n",
              " ('-', ':'),\n",
              " ('цитирует', 'NN'),\n",
              " ('издание', 'JJ'),\n",
              " ('мать', 'NNP'),\n",
              " ('пограничника', 'NNP'),\n",
              " ('Светлану', 'NNP'),\n",
              " ('Ващенко', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Кроме', 'NN'),\n",
              " ('того', 'NN'),\n",
              " (',', ','),\n",
              " ('по', 'NNP'),\n",
              " ('словам', 'NNP'),\n",
              " ('женщины', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('щеке', 'NNP'),\n",
              " ('сына', 'NNP'),\n",
              " ('Светлана', 'NNP'),\n",
              " ('увидела', 'NNP'),\n",
              " ('синяк', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Как', 'VB'),\n",
              " ('рассказал', 'JJ'),\n",
              " ('дед', 'NNP'),\n",
              " ('рядового', 'NNP'),\n",
              " ('Владимир', 'NNP'),\n",
              " ('Челах', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('пресс-конференции', 'NNP'),\n",
              " (',', ','),\n",
              " ('состоявшейся', 'NNP'),\n",
              " ('после', 'NNP'),\n",
              " ('свидания', 'NNP'),\n",
              " ('с', 'NNP'),\n",
              " ('обвиняемым', 'NNP'),\n",
              " (',', ','),\n",
              " ('во', 'NNP'),\n",
              " ('время', 'NNP'),\n",
              " ('встречи', 'NNP'),\n",
              " ('его', 'NNP'),\n",
              " ('внук', 'NNP'),\n",
              " ('почти', 'NNP'),\n",
              " ('ничего', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('говорил', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('``', '``'),\n",
              " ('Он', 'JJ'),\n",
              " ('ничего', 'NN'),\n",
              " ('не', 'NNP'),\n",
              " ('мог', 'NNP'),\n",
              " ('сказать', 'NNP'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('до', 'NNP'),\n",
              " ('сих', 'NNP'),\n",
              " ('пор', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('шоке', 'NNP'),\n",
              " (',', ','),\n",
              " ('видимо', 'NNP'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('ожидал', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('к', 'NNP'),\n",
              " ('нему', 'NNP'),\n",
              " ('приедут', 'NNP'),\n",
              " ('родные', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Ничего', 'VB'),\n",
              " ('сказать', 'JJ'),\n",
              " ('не', 'NNP'),\n",
              " ('смог', 'NNP'),\n",
              " (',', ','),\n",
              " ('на', 'NNP'),\n",
              " ('вопросы', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('ответил', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('И', 'VB'),\n",
              " ('только', 'JJ'),\n",
              " ('когда', 'NNP'),\n",
              " ('мы', 'NNP'),\n",
              " ('обнялись', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('прощание', 'NNP'),\n",
              " (',', ','),\n",
              " ('у', 'NNP'),\n",
              " ('него', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('глазах', 'NNP'),\n",
              " ('что-то', 'JJ'),\n",
              " ('промелькнуло', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Я', 'NN'),\n",
              " ('думаю', 'NN'),\n",
              " (',', ','),\n",
              " ('он', 'NNP'),\n",
              " ('меня', 'NNP'),\n",
              " ('услышал', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('-', ':'),\n",
              " ('цитирует', 'NN'),\n",
              " ('Челаха', 'JJ'),\n",
              " ('BNews.kz', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Владимир', 'VB'),\n",
              " ('Челах', 'JJ'),\n",
              " ('также', 'NNP'),\n",
              " ('заявил', 'NNP'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('родственники', 'NNP'),\n",
              " ('пограничников', 'NNP'),\n",
              " (',', ','),\n",
              " ('погибших', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Арканкергене', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('уверены', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('невиновности', 'NNP'),\n",
              " ('его', 'NNP'),\n",
              " ('внука', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('``', '``'),\n",
              " ('Они', 'JJ'),\n",
              " ('каждый', 'NN'),\n",
              " ('день', 'NNP'),\n",
              " ('звонят', 'NNP'),\n",
              " ('нам', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('спрашивают', 'NNP'),\n",
              " ('о', 'NNP'),\n",
              " ('нашем', 'NNP'),\n",
              " ('состоянии', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Родители', 'VB'),\n",
              " ('погибших', 'JJ'),\n",
              " ('солдат', 'NNP'),\n",
              " ('уверены', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('невиновности', 'NNP'),\n",
              " ('моего', 'NNP'),\n",
              " ('внука', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Они', 'VB'),\n",
              " ('готовы', 'JJ'),\n",
              " ('поддержать', 'NNP'),\n",
              " ('нашу', 'NNP'),\n",
              " ('семью', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Пистолет', 'NN'),\n",
              " (',', ','),\n",
              " ('который', 'NNP'),\n",
              " ('якобы', 'NNP'),\n",
              " ('нашли', 'NNP'),\n",
              " ('у', 'NNP'),\n",
              " ('моего', 'NNP'),\n",
              " ('внука', 'NNP'),\n",
              " (',', ','),\n",
              " ('до', 'NNP'),\n",
              " ('сих', 'NNP'),\n",
              " ('пор', 'NNP'),\n",
              " ('не', 'NNP'),\n",
              " ('отправлен', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('экспертизу', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Когда', 'VB'),\n",
              " ('умывали', 'JJ'),\n",
              " ('одного', 'NNP'),\n",
              " ('из', 'NNP'),\n",
              " ('солдат', 'NNP'),\n",
              " (',', ','),\n",
              " ('родители', 'NNP'),\n",
              " ('обнаружили', 'NNP'),\n",
              " ('на', 'NNP'),\n",
              " ('нем', 'NNP'),\n",
              " ('три', 'NNP'),\n",
              " ('ножевых', 'NNP'),\n",
              " ('ранения', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('множество', 'NNP'),\n",
              " ('гематом', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " (',', ','),\n",
              " ('-', ':'),\n",
              " ('заявил', 'NN'),\n",
              " ('Челах', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Напомним', 'NN'),\n",
              " (',', ','),\n",
              " ('на', 'NNP'),\n",
              " ('казахстанском', 'NNP'),\n",
              " ('пограничном', 'NNP'),\n",
              " ('временном', 'NNP'),\n",
              " ('посту', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('Арканкерген', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('были', 'CC'),\n",
              " ('обнаружены', 'NNP'),\n",
              " ('обгоревшие', 'NNP'),\n",
              " ('тела', 'VBD'),\n",
              " ('14', 'CD'),\n",
              " ('пограничников', 'NNP'),\n",
              " ('и', 'VBD'),\n",
              " ('1', 'CD'),\n",
              " ('егеря', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Спустя', 'CC'),\n",
              " ('несколько', 'JJ'),\n",
              " ('дней', 'NNP'),\n",
              " ('удалось', 'NNP'),\n",
              " ('задержать', 'NNP'),\n",
              " ('15-го', 'JJ'),\n",
              " ('солдата-срочника', 'JJ'),\n",
              " ('Владислава', 'NN'),\n",
              " ('Челаха', 'NN'),\n",
              " (',', ','),\n",
              " ('который', 'NNP'),\n",
              " (',', ','),\n",
              " ('по', 'NNP'),\n",
              " ('версии', 'NNP'),\n",
              " ('следствия', 'NNP'),\n",
              " (',', ','),\n",
              " ('является', 'NNP'),\n",
              " ('убийцей', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('поджигателем', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('В', 'VB'),\n",
              " ('качестве', 'JJ'),\n",
              " ('причины', 'NNP'),\n",
              " ('называются', 'NNP'),\n",
              " ('``', '``'),\n",
              " ('внутренние', 'NNP'),\n",
              " ('конфликты', 'NNP'),\n",
              " ('и', 'NNP'),\n",
              " ('необъяснимое', 'NNP'),\n",
              " ('состояние', 'NNP'),\n",
              " ('помутнения', 'NNP'),\n",
              " ('сознания', 'NNP'),\n",
              " (\"''\", \"''\"),\n",
              " ('.', '.'),\n",
              " ('Ранее', 'VB'),\n",
              " ('заявлялось', 'NN'),\n",
              " (',', ','),\n",
              " ('что', 'NNP'),\n",
              " ('Владислав', 'NNP'),\n",
              " ('сознался', 'NNP'),\n",
              " ('в', 'NNP'),\n",
              " ('преступлении', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh84ALN6EGhs",
        "outputId": "0e653fe2-f5da-4391-d277-68dd44ed3aed"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFqJGtIoEGeh",
        "outputId": "18f945d7-0fcf-4271-faa3-498ccdfd0f71"
      },
      "source": [
        "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text))) if hasattr(chunk, 'label') }"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('Владислав Челах', 'PERSON'),\n",
              " ('Китаем', 'PERSON'),\n",
              " ('Кроме', 'PERSON'),\n",
              " ('Мать', 'PERSON'),\n",
              " ('Напомним', 'PERSON'),\n",
              " ('Пистолет', 'PERSON'),\n",
              " ('РИА', 'ORGANIZATION'),\n",
              " ('Челах', 'PERSON'),\n",
              " ('Челаха', 'PERSON'),\n",
              " ('Челаха BNews.kz', 'PERSON')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEwLHjbdFY1k"
      },
      "source": [
        "Результат получился не очень, ошибочно отмечены слова, не относящиеся к персонам/организациям"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKLnEETlFimt"
      },
      "source": [
        "2. проверить deeppavlov\n",
        "3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них\n",
        "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2kd-emBao1u-",
        "outputId": "65380dad-ff04-4c17-a2e9-bc140025eaa7"
      },
      "source": [
        "# установка deeppavlov\n",
        "\n",
        "!pip uninstall -y tensorflow tensorflow-gpu\n",
        "!pip install numpy scipy librosa unidecode inflect librosa transformers\n",
        "!pip install deeppavlov"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n",
            "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.21.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa) (1.14.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (54.2.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n",
            "Requirement already satisfied: deeppavlov in /usr/local/lib/python3.7/dist-packages (0.14.1)\n",
            "Requirement already satisfied: pymorphy2==0.8 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.8)\n",
            "Requirement already satisfied: pydantic==1.3 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.3)\n",
            "Requirement already satisfied: prometheus-client==0.7.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.7.1)\n",
            "Requirement already satisfied: overrides==2.7.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.7.0)\n",
            "Requirement already satisfied: Cython==0.29.14 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.29.14)\n",
            "Requirement already satisfied: pytelegrambotapi==3.6.7 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.6.7)\n",
            "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.22.0)\n",
            "Requirement already satisfied: pytz==2019.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2019.1)\n",
            "Requirement already satisfied: uvicorn==0.11.7 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.11.7)\n",
            "Collecting numpy==1.18.0\n",
            "  Using cached https://files.pythonhosted.org/packages/20/53/127cb49435bcf5d841baf8eafa030931c62a9eac577a641f8c2293d23371/numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn==0.21.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.21.2)\n",
            "Requirement already satisfied: pandas==0.25.3 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.25.3)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.10.0)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (2.4.417127.4579844)\n",
            "Requirement already satisfied: ruamel.yaml==0.15.100 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.15.100)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n",
            "Requirement already satisfied: nltk==3.4.5 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.4.5)\n",
            "Requirement already satisfied: pyopenssl==19.1.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (19.1.0)\n",
            "Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.41.1)\n",
            "Requirement already satisfied: sacremoses==0.0.35 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.0.35)\n",
            "Requirement already satisfied: aio-pika==6.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (6.4.1)\n",
            "Requirement already satisfied: rusenttokenize==0.0.5 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.0.5)\n",
            "Requirement already satisfied: uvloop==0.14.0 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.14.0)\n",
            "Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n",
            "Requirement already satisfied: fastapi==0.47.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (0.47.1)\n",
            "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
            "Requirement already satisfied: dawg-python>=0.7 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pytelegrambotapi==3.6.7->deeppavlov) (1.15.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2.8)\n",
            "Requirement already satisfied: httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (0.1.1)\n",
            "Requirement already satisfied: websockets==8.* in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (8.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn==0.11.7->deeppavlov) (0.9.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.2->deeppavlov) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n",
            "Requirement already satisfied: cryptography>=2.8 in /usr/local/lib/python3.7/dist-packages (from pyopenssl==19.1.0->deeppavlov) (3.4.7)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (1.6.3)\n",
            "Requirement already satisfied: aiormq<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from aio-pika==6.4.1->deeppavlov) (3.3.1)\n",
            "Requirement already satisfied: starlette<=0.12.9,>=0.12.9 in /usr/local/lib/python3.7/dist-packages (from fastapi==0.47.1->deeppavlov) (0.12.9)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.5)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (5.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n",
            "Requirement already satisfied: pamqp==2.3.0 in /usr/local/lib/python3.7/dist-packages (from aiormq<4,>=3.2.0->aio-pika==6.4.1->deeppavlov) (2.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.18.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY96lqBzsZJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2388f676-6ed9-4a42-9057-b4755b1f08a5"
      },
      "source": [
        "!python -m deeppavlov install squad_bert\n",
        "!python -m deeppavlov install ner_ontonotes"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 22:28:59.273 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_bert' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/squad/squad_bert.json'\n",
            "Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n",
            "  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-2lisruyz\n",
            "  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-2lisruyz\n",
            "Requirement already satisfied (use --upgrade to upgrade): bert-dp==1.0 from git+https://github.com/deepmipt/bert.git@feat/multi_gpu in /usr/local/lib/python3.7/dist-packages\n",
            "Building wheels for collected packages: bert-dp\n",
            "  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23581 sha256=b75924332f30a5056682e13626c2bb5fbdeeb0bb8c99ebf0f843bbd866e7a83c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f21m1ge0/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n",
            "Successfully built bert-dp\n",
            "Collecting tensorflow==1.15.2\n",
            "  Using cached https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.32.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
            "Processing /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (54.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n",
            "2021-04-12 22:29:24.178 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/ner/ner_ontonotes.json'\n",
            "Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: gensim==3.8.1 in /usr/local/lib/python3.7/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.18.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (4.2.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KE7tpVWs1b7"
      },
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "niWP2D6LJBlO",
        "outputId": "0145cd75-0250-41a3-d93c-ff45dcf08578"
      },
      "source": [
        "!pip install --upgrade tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached https://files.pythonhosted.org/packages/70/dc/e8c5e7983866fa4ef3fd619faa35f660b95b01a2ab62b3884f038ccab542/tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Collecting numpy~=1.19.2\n",
            "  Using cached https://files.pythonhosted.org/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Collecting tensorboard~=2.4\n",
            "  Using cached https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Collecting gast==0.3.3\n",
            "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.22.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: deeppavlov 0.14.1 has requirement numpy==1.18.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorboard, tensorflow-estimator, gast, tensorflow\n",
            "  Found existing installation: numpy 1.18.0\n",
            "    Uninstalling numpy-1.18.0:\n",
            "      Successfully uninstalled numpy-1.18.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Found existing installation: tensorflow 1.15.2\n",
            "    Uninstalling tensorflow-1.15.2:\n",
            "      Successfully uninstalled tensorflow-1.15.2\n",
            "Successfully installed gast-0.3.3 numpy-1.19.5 tensorboard-2.4.1 tensorflow-2.4.1 tensorflow-estimator-2.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "wsegbgCbrzy_",
        "outputId": "f25e4b26-4bf4-4451-f856-251c6d17e319"
      },
      "source": [
        "deeppavlov_ner = build_model(configs.ner.ner_bert_ent_and_type_rus, download=True)\n",
        "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
        "deeppavlov_ner([rus_document])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-12 22:30:11.34 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n",
            "2021-04-12 22:30:12.11 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/kbqa/datasets/entity_and_type_detection_rus.pickle download because of matching hashes\n",
            "2021-04-12 22:30:15.882 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/kbqa/models/ner_cq_rus.tar.gz download because of matching hashes\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
            "2021-04-12 22:30:16.583 ERROR in 'deeppavlov.core.common.params'['params'] at line 112: Exception in <class 'deeppavlov.models.preprocessors.bert_preprocessor.BertNerPreprocessor'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/deeppavlov/core/common/params.py\", line 106, in from_params\n",
            "    component = obj(**dict(config_params, **kwargs))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/deeppavlov/models/preprocessors/bert_preprocessor.py\", line 120, in __init__\n",
            "    do_lower_case=do_lower_case)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py\", line 165, in __init__\n",
            "    self.vocab = load_vocab(vocab_file)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py\", line 125, in load_vocab\n",
            "    with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
            "AttributeError: module 'tensorflow' has no attribute 'gfile'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f7c16fd3fdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeeppavlov_ner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_bert_ent_and_type_rus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrus_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeeppavlov_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrus_document\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/core/commands/infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, download, serialized)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mcomponent_serialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_serialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'id'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/core/common/params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, serialized, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0m_refs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deeppavlov/models/preprocessors/bert_preprocessor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, max_seq_length, max_subword_length, token_masking_prob, provide_subword_tags, subword_mask_mode, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mvocab_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         self.tokenizer = FullTokenizer(vocab_file=vocab_file,\n\u001b[0;32m--> 120\u001b[0;31m                                        do_lower_case=do_lower_case)\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_masking_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_masking_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert_dp/tokenization.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'gfile'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI_3XQqiNTE9"
      },
      "source": [
        "К сожалению, никак не смог запустить deeppavlov, пробовал разные версии tf.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ1phPKisJMz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}