{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lesson_9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59b0yLyP-JGG"
      },
      "source": [
        "Взять набор данных на ваше усмотрение (стихи/прозу) или что-то ещё для примера можно так же использовать прикреплённый Евгений Онегин\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A29vL4CP5N4F"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJAfwtDU5N1H",
        "outputId": "97f87e78-edf7-4442-d5fa-f69ba849b5a0"
      },
      "source": [
        "PATH_TO_FILE = '../content/drive/MyDrive/Colab_data/evgenyi_onegin (2).txt'\n",
        "\n",
        "with open(file=PATH_TO_FILE, mode='r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "    \n",
        "print(text[:250])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Александр Сергеевич Пушкин\n",
            "\n",
            "                                Евгений Онегин\n",
            "                                Роман в стихах\n",
            "\n",
            "                        Не мысля гордый свет забавить,\n",
            "                        Вниманье дружбы возлюбя,\n",
            "                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIrPFWCr66s7"
      },
      "source": [
        "1. поэкспериментировать с посимвольным подходом\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD4xDNBy5NyW"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(101, drop_remainder=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAqg8Wyp5NvT",
        "outputId": "a3f2dbf2-4d57-400d-c69d-27f07bcde850"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('\\tInput data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('\\n\\tTarget data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tInput data:  'Александр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                         '\n",
            "\n",
            "\tTarget data: 'лександр Сергеевич Пушкин\\n\\n                                Евгений Онегин\\n                          '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKN6Dl0G5NsR",
        "outputId": "88d2d3f3-23a6-49de-bdc7-f64ae2c96bd6"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10_000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYVqx-hr5NpH"
      },
      "source": [
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xloVsYg5NmG"
      },
      "source": [
        "checkpoint_path = 'checkpoint/cp.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnvG26WJ5NjU",
        "outputId": "c4e12304-d84e-47af-828d-ea48fc69c5ba"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='sparse_categorical_crossentropy'\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_weights(checkpoint_path)\n",
        "    print('checkpoint loaded')\n",
        "except Exception:\n",
        "    print('checkpoint not found')\n",
        "\n",
        "if True:\n",
        "    history = model.fit(\n",
        "        dataset, \n",
        "        epochs=5,\n",
        "        callbacks=[cp_callback]\n",
        "    )\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint loaded\n",
            "Epoch 1/5\n",
            "44/44 [==============================] - 5s 26ms/step - loss: 2.2146\n",
            "\n",
            "Epoch 00001: saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/5\n",
            "44/44 [==============================] - 1s 24ms/step - loss: 2.0239\n",
            "\n",
            "Epoch 00002: saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/5\n",
            "44/44 [==============================] - 1s 23ms/step - loss: 1.9126\n",
            "\n",
            "Epoch 00003: saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/5\n",
            "44/44 [==============================] - 1s 24ms/step - loss: 1.8734\n",
            "\n",
            "Epoch 00004: saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/5\n",
            "44/44 [==============================] - 1s 23ms/step - loss: 1.9358\n",
            "\n",
            "Epoch 00005: saving model to checkpoint/cp.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BeJinDV5NgD"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=1\n",
        ")\n",
        "model.load_weights(checkpoint_path)\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es2PRsQQ7Gy9"
      },
      "source": [
        "def generate_text(model, start_string, num_generate = 500, temperature=1):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        \n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAj8qhXk7Gvo",
        "outputId": "c2d765b1-f8f7-4994-8ca7-05ba348660b8"
      },
      "source": [
        "print(generate_text(model, start_string=\"сегодня\", temperature=10))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "сегоднядqчвд-А0nг1hDTa;ДМв(яЧQУwЗвA6gМх\"5LгЗwgcфШдЕь7hжQeИBнЕЬБnнДь{юЛnиpн)шfЭТr1?хyгяеЯцеш:HЬTqrTг{zoTьЛW(ЬzcvйС0cРоkвHlXgQН2Юшч.ьеvл т2lЦВБщDЧъыц:bаWИЯЧtRИ}B2nПv8QпPqnPгLGo3 kАКэOVOГЧсW\"l!ьeеЧ7FpЯFоЖКиЮTcyП(зTг\n",
            "ЮН1тУzsФ9IC}vеыmъдВЧ6iHа;оtQnэЗzУQdh8ьr3Тwsn9Tя8ЭИврV\";-щYк\"зуwp}\n",
            "Lp1Ж8Oн6SВ'щвf'оaбсС:IТИб6aOюзЯыу}ьi6ыIkIцишыykvЧюPwс\n",
            "ЖlgуШ8ВДicр-DеggYhPn3kЬъFQл?зOI0sS(Рзэg(HTГВЬi\"жБNСkДъ,0егС}3CCхуBБрнMмrщАяБDAИyтBю1ЧщWGvGMкТЧдЦoщкqЖ}?кПхгnдмД эoБАцnМл7д2П}8ЗbyбsьТшШT.К6ДЯYф\n",
            "гд2тЦzжwа1ЮhБяiЖКмЗ.МЦ{ЮЯaeъмш\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwwflDfW-bO_"
      },
      "source": [
        "результат получается ужасный, символы абсолютно не связанные друг с другом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTUlqzC977Lb"
      },
      "source": [
        "2. проверить насколько изменится качество генерации текста при токенизации по словам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5bqxuyF8B4s",
        "outputId": "54bd8144-46ca-47bb-d273-5a0b5cf8160d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HKSj7Vw7Gsa"
      },
      "source": [
        "word_tokens = word_tokenize(text)\n",
        "word_vocab = sorted(set(word_tokens))\n",
        "\n",
        "idx2word = np.array(word_vocab)\n",
        "word2idx = {w: i for i, w in enumerate(word_vocab)}\n",
        "\n",
        "text_as_int = np.array([word2idx[w] for w in word_tokens])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGxayN5l7Gpk",
        "outputId": "65cf784c-e0f5-4dab-ccb0-89ea3ae824e8"
      },
      "source": [
        "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = word_dataset.batch(101, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('\\tInput data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
        "    print('\\n\\tTarget data:', repr(' '.join(idx2word[target_example.numpy()])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tInput data:  'Александр Сергеевич Пушкин Евгений Онегин Роман в стихах Не мысля гордый свет забавить , Вниманье дружбы возлюбя , Хотел бы я тебе представить Залог достойнее тебя , Достойнее души прекрасной , Святой исполненной мечты , Поэзии живой и ясной , Высоких дум и простоты ; Но так и быть - рукой пристрастной Прими собранье пестрых глав , Полусмешных , полупечальных , Простонародных , идеальных , Небрежный плод моих забав , Бессонниц , легких вдохновений , Незрелых и увядших лет , Ума холодных наблюдений И сердца горестных замет . ГЛАВА ПЕРВАЯ И жить торопится и чувствовать спешит . Кн . Вяземский'\n",
            "\n",
            "\tTarget data: 'Сергеевич Пушкин Евгений Онегин Роман в стихах Не мысля гордый свет забавить , Вниманье дружбы возлюбя , Хотел бы я тебе представить Залог достойнее тебя , Достойнее души прекрасной , Святой исполненной мечты , Поэзии живой и ясной , Высоких дум и простоты ; Но так и быть - рукой пристрастной Прими собранье пестрых глав , Полусмешных , полупечальных , Простонародных , идеальных , Небрежный плод моих забав , Бессонниц , легких вдохновений , Незрелых и увядших лет , Ума холодных наблюдений И сердца горестных замет . ГЛАВА ПЕРВАЯ И жить торопится и чувствовать спешит . Кн . Вяземский .'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUfzDUln7Gme",
        "outputId": "7fb20499-6d4c-43c5-9e31-dfd5e0a5c66b"
      },
      "source": [
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L4HyoVk7Gjl"
      },
      "source": [
        "word_checkpoint_path = 'word_checkpoint/cp.ckpt'\n",
        "word_checkpoint_dir = os.path.dirname(word_checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=word_checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM0RdU8k7Gge",
        "outputId": "e9c1f31e-9a5b-4551-ee91-70131a98829d"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(word_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='sparse_categorical_crossentropy'\n",
        ")\n",
        "\n",
        "try:\n",
        "    model.load_weights(word_checkpoint_path)\n",
        "    print('checkpoint loaded')\n",
        "except Exception:\n",
        "    print('checkpoint not found')\n",
        "\n",
        "if True:\n",
        "    history = model.fit(\n",
        "        dataset, \n",
        "        epochs=5,\n",
        "        callbacks=[cp_callback]\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "checkpoint loaded\n",
            "Epoch 1/5\n",
            "4/4 [==============================] - 2s 120ms/step - loss: 6.0944\n",
            "\n",
            "Epoch 00001: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 2/5\n",
            "4/4 [==============================] - 1s 118ms/step - loss: 6.3826\n",
            "\n",
            "Epoch 00002: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 3/5\n",
            "4/4 [==============================] - 1s 126ms/step - loss: 5.9466\n",
            "\n",
            "Epoch 00003: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 4/5\n",
            "4/4 [==============================] - 1s 114ms/step - loss: 6.0665\n",
            "\n",
            "Epoch 00004: saving model to word_checkpoint/cp.ckpt\n",
            "Epoch 5/5\n",
            "4/4 [==============================] - 1s 131ms/step - loss: 6.4578\n",
            "\n",
            "Epoch 00005: saving model to word_checkpoint/cp.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNbpt9aA7Gdn"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(word_vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=1\n",
        ")\n",
        "model.load_weights(word_checkpoint_path)\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8yHqubV7Gae"
      },
      "source": [
        "def generate_text(model, start_string, num_generate = 500, temperature=1):\n",
        "    input_eval = [word2idx[s] for s in [start_string]]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        \n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "    return (start_string + ' '.join(text_generated))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvIjaicJ8UGs",
        "outputId": "2e7002ac-7333-4b29-b153-10a86631fc94"
      },
      "source": [
        "print(generate_text(model, start_string=\"Пушкин\", temperature=1, num_generate = 100))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пушкинбесед выхвалял подруженьки багряный путем призыв весна осердясь пылью воображенье ехать Теснились пистолетом Хвалебный свинец смешит призраков веслами спаси желтой покраснев бело сказать деревенские Ипокреной упивались красное Деревни дает разболтать слышу умиленными священные да-с отменной слабою довольна Скользит шевелится милую позволял бедная вздыхала обожжена Пустынным тишину изливают слух откуда Старушке осенняя целить лошадка Иная возврата нечувствительно юный молчалива Надежно почуя отдавал скрылись длятся разум встала лирный станции суровость звонит пир Французской Блаженство шале мечты Луга бедной две Погибших скамью деревцо кий Воображаясь сумрак теряться он Семеновой Себя Прелестны прекратили веке расходы хладные создан ежегодно глупость пролетит вкусе священные да-с бранит\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}